# This .yaml file for "ensemble test"
# each models are used for ensemble

# model order
#   size of embedded network input dimension
#   size of decoder network input dimension
#   size of encoder netowrk input dimension
#   encoder network type (resnet(res), wide_resnet(wide_res), resnext)
#   saved file number 'decoder{}'.format(load_model_name) or 'encoder{}'.format(load_model_name)

#model1:
#  emb_dim: 512
#  attention_dim: 512
#  decoder_dim: 512
#  encoder_type: wide_res
#  load_model_name: 0
##Wide ResNet101-2
##PubChem2
#
#
#model2:
#  emb_dim: 512
#  attention_dim: 512
#  decoder_dim: 512
#  encoder_type: wide_res
#  load_model_name: 1
##Wide ResNet101-2
##PubChem1
#
#
#model3:
#  emb_dim: 512
#  attention_dim: 512
#  decoder_dim: 512
#  encoder_type: res
#  load_model_name: 2
##ResNet152 DACON dataset
#
#model4:
#  emb_dim: 256
#  attention_dim: 256
#  decoder_dim: 256
#  encoder_type: res
#  load_model_name: 3
#  #ResNet152 DACON dataset
#
#model5: #resnext
#  emb_dim: 256
#  attention_dim: 256
#  decoder_dim: 256
#  encoder_type: resnext
#  load_model_name: 4
#  #ResNeXt-101-32x8d
#  #PubChem1

#lg+PubChem1M+ChEMBL75+RDKit_clear: <75
model1:
  emb_dim: 512
  attention_dim: 512
  decoder_dim: 512
  encoder_type: efficientnetB0
  tf_encoder: 6
  tf_decoder: 6
  load_model_path: /home/zihanchen/results_training/multi_lg+pubchem+chembl+rdkitclear/E0+6TFE+6TFD/graph_save/model-emb_dim_512-attention_dim_512-decoder_dim_512-dropout_0.5-batch_size_512/
  load_model_num: 9

##lg+PubChem+RDKit_clear+RDKit_noise: <75
#model2:
#  emb_dim: 512
#  attention_dim: 512
#  decoder_dim: 512
#  encoder_type: efficientnetB0
#  tf_encoder: 2
#  tf_decoder: 6
#  load_model_path: /home/zihanchen/results_training/multi_lg+pubchem+rdkitclear_noise/E0+2TFE+LSTM/graph_save/model-emb_dim_512-attention_dim_512-decoder_dim_512-dropout_0.5-batch_size_512/
#  load_model_num: 16
#
##lg+PubChem+RDKit_clear+RDKit_noise: <75
#model3:
#  emb_dim: 512
#  attention_dim: 512
#  decoder_dim: 512
#  encoder_type: efficientnetB0
#  tf_encoder: 2
#  tf_decoder: 6
#  load_model_path: /home/zihanchen/results_training/multi_lg+pubchem+rdkitclear_noise/E0+2TFE+6TFD/graph_save/model-emb_dim_512-attention_dim_512-decoder_dim_512-dropout_0.5-batch_size_512/
#  load_model_num: 16
#
##lg+PubChem: <100
#model4:
#  emb_dim: 512
#  attention_dim: 512
#  decoder_dim: 512
#  encoder_type: efficientnetB0
#  tf_encoder: 2
#  tf_decoder: 0
#  load_model_path: /home/zihanchen/results_training/multi_lg+pubchem+chembl+rdkitclear/E0+6TFE+6TFD/graph_save/model-emb_dim_512-attention_dim_512-decoder_dim_512-dropout_0.5-batch_size_512/
#  load_model_num: 9
